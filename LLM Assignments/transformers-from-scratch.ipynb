{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"toc_visible":true,"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This is the implementation of Transformers using PyTorch. This was an assignment for my LLMs course.","metadata":{"_uuid":"b630082d-c5c3-40df-913e-af071ff87727","_cell_guid":"4e93ce39-a9d4-401f-8108-ce0e55477c0d","id":"3a4cd103","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-08T02:08:12.203964Z","iopub.execute_input":"2024-06-08T02:08:12.204307Z","iopub.status.idle":"2024-06-08T02:08:12.208861Z","shell.execute_reply.started":"2024-06-08T02:08:12.204280Z","shell.execute_reply":"2024-06-08T02:08:12.207836Z"}}},{"cell_type":"code","source":"# Make sure you have the following packages installed:\n!pip install spacy torchtext portalocker --quiet","metadata":{"_uuid":"f62ed9aa-da5f-441d-b650-31a3522ba445","_cell_guid":"475c5aec-9732-4db9-a8f0-91722cfc72d5","collapsed":false,"id":"8b6bbb42","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-08T02:08:12.210637Z","iopub.execute_input":"2024-06-08T02:08:12.210904Z","iopub.status.idle":"2024-06-08T02:08:25.804433Z","shell.execute_reply.started":"2024-06-08T02:08:12.210878Z","shell.execute_reply":"2024-06-08T02:08:25.803278Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"!pip install torch==2.1.0 torchvision==0.12.0 torchaudio==0.11.0\n!pip install torchdata==0.7.1 torchtext==0.16.0\n!pip install dill","metadata":{"_uuid":"a36b9980-8fc5-4301-8fe4-a15046d2613b","_cell_guid":"8d4f53a4-0aa6-4996-b719-5d654fb2dbe4","collapsed":false,"id":"Op4r4ExK7xFs","outputId":"8b058d55-8e2f-4c8b-9d4e-eed484715ba7","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-08T02:08:25.805814Z","iopub.execute_input":"2024-06-08T02:08:25.806112Z","iopub.status.idle":"2024-06-08T02:08:43.779065Z","shell.execute_reply.started":"2024-06-08T02:08:25.806084Z","shell.execute_reply":"2024-06-08T02:08:43.777888Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Collecting torch==2.1.0\n  Downloading torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\nCollecting torchvision==0.12.0\n  Downloading torchvision-0.12.0-cp310-cp310-manylinux1_x86_64.whl.metadata (10 kB)\nCollecting torchaudio==0.11.0\n  Downloading torchaudio-0.11.0-cp310-cp310-manylinux1_x86_64.whl.metadata (1.0 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0) (2024.3.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.1.0)\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.1.0)\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.1.0)\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.1.0)\n  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.1.0)\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.1.0)\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.1.0)\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.1.0)\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.1.0)\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.18.1 (from torch==2.1.0)\n  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.1.0)\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton==2.1.0 (from torch==2.1.0)\n  Downloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision==0.12.0) (1.26.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision==0.12.0) (2.32.3)\nINFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n\u001b[31mERROR: Cannot install torch==2.1.0 and torchvision==0.12.0 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n\u001b[0m\nThe conflict is caused by:\n    The user requested torch==2.1.0\n    torchvision 0.12.0 depends on torch==1.11.0\n\nTo fix this you could try to:\n1. loosen the range of package versions you've specified\n2. remove package versions to allow pip attempt to solve the dependency conflict\n\n\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n\u001b[0mRequirement already satisfied: torchdata==0.7.1 in /opt/conda/lib/python3.10/site-packages (0.7.1)\nCollecting torchtext==0.16.0\n  Downloading torchtext-0.16.0-cp310-cp310-manylinux1_x86_64.whl.metadata (7.5 kB)\nRequirement already satisfied: urllib3>=1.25 in /opt/conda/lib/python3.10/site-packages (from torchdata==0.7.1) (1.26.18)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchdata==0.7.1) (2.32.3)\nRequirement already satisfied: torch>=2 in /opt/conda/lib/python3.10/site-packages (from torchdata==0.7.1) (2.1.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torchtext==0.16.0) (4.66.4)\nCollecting torch>=2 (from torchdata==0.7.1)\n  Using cached torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchtext==0.16.0) (1.26.4)\nINFO: pip is looking at multiple versions of torchtext to determine which version is compatible with other requirements. This could take a while.\n\u001b[31mERROR: Cannot install torchdata==0.7.1 and torchtext==0.16.0 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n\u001b[0m\nThe conflict is caused by:\n    The user requested torchdata==0.7.1\n    torchtext 0.16.0 depends on torchdata==0.7.0\n\nTo fix this you could try to:\n1. loosen the range of package versions you've specified\n2. remove package versions to allow pip attempt to solve the dependency conflict\n\n\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n\u001b[0mRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (0.3.8)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install dill","metadata":{"_uuid":"ad215cc4-6cc6-45f4-b0fc-e99ed68af09d","_cell_guid":"11eb0306-8e64-4651-ae6f-2fbf6a600a51","collapsed":false,"id":"S97jBPIp8rE9","outputId":"a72300cd-c891-43d2-be11-e0d1268dca58","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-08T02:08:43.780663Z","iopub.execute_input":"2024-06-08T02:08:43.780980Z","iopub.status.idle":"2024-06-08T02:08:55.940638Z","shell.execute_reply.started":"2024-06-08T02:08:43.780952Z","shell.execute_reply":"2024-06-08T02:08:55.939675Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (0.3.8)\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install torchdata==0.7.1 torchtext==0.16.0","metadata":{"_uuid":"a92a0f42-b43a-4821-b364-b7a87c32fca6","_cell_guid":"a8dc832f-7f74-4822-97b3-1a79ec3c5ccf","collapsed":false,"id":"keSfdwAQ721l","outputId":"c837a1a4-e473-41a3-ccf7-2a57d0d72dc6","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-08T02:08:55.943355Z","iopub.execute_input":"2024-06-08T02:08:55.943712Z","iopub.status.idle":"2024-06-08T02:08:58.031052Z","shell.execute_reply.started":"2024-06-08T02:08:55.943682Z","shell.execute_reply":"2024-06-08T02:08:58.029986Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchdata==0.7.1 in /opt/conda/lib/python3.10/site-packages (0.7.1)\nCollecting torchtext==0.16.0\n  Using cached torchtext-0.16.0-cp310-cp310-manylinux1_x86_64.whl.metadata (7.5 kB)\nRequirement already satisfied: urllib3>=1.25 in /opt/conda/lib/python3.10/site-packages (from torchdata==0.7.1) (1.26.18)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchdata==0.7.1) (2.32.3)\nRequirement already satisfied: torch>=2 in /opt/conda/lib/python3.10/site-packages (from torchdata==0.7.1) (2.1.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torchtext==0.16.0) (4.66.4)\nCollecting torch>=2 (from torchdata==0.7.1)\n  Using cached torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchtext==0.16.0) (1.26.4)\nINFO: pip is looking at multiple versions of torchtext to determine which version is compatible with other requirements. This could take a while.\n\u001b[31mERROR: Cannot install torchdata==0.7.1 and torchtext==0.16.0 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n\u001b[0m\nThe conflict is caused by:\n    The user requested torchdata==0.7.1\n    torchtext 0.16.0 depends on torchdata==0.7.0\n\nTo fix this you could try to:\n1. loosen the range of package versions you've specified\n2. remove package versions to allow pip attempt to solve the dependency conflict\n\n\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install torchdata","metadata":{"_uuid":"fb734020-5272-4dd5-9d40-f0946bcc531c","_cell_guid":"80510031-8a59-4920-95f8-f9c33d47c342","collapsed":false,"id":"3TI558dm5YI-","outputId":"af35db6c-cd3b-482a-a01b-ced354354419","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-08T02:08:58.032675Z","iopub.execute_input":"2024-06-08T02:08:58.033045Z","iopub.status.idle":"2024-06-08T02:09:10.013368Z","shell.execute_reply.started":"2024-06-08T02:08:58.033010Z","shell.execute_reply":"2024-06-08T02:09:10.012236Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchdata in /opt/conda/lib/python3.10/site-packages (0.7.1)\nRequirement already satisfied: urllib3>=1.25 in /opt/conda/lib/python3.10/site-packages (from torchdata) (1.26.18)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchdata) (2.32.3)\nRequirement already satisfied: torch>=2 in /opt/conda/lib/python3.10/site-packages (from torchdata) (2.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=2->torchdata) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=2->torchdata) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=2->torchdata) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=2->torchdata) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=2->torchdata) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=2->torchdata) (2024.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchdata) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchdata) (3.6)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchdata) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=2->torchdata) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=2->torchdata) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport math\nimport spacy\nimport os\nimport numpy as np\n\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\nfrom torchtext.datasets import Multi30k\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader","metadata":{"_uuid":"f2c572ec-0749-4f12-85b0-4fba9332a2c6","_cell_guid":"9332f45e-cb75-46e0-8378-40d31b9dee24","collapsed":false,"id":"62a5dee5","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-08T02:09:10.014827Z","iopub.execute_input":"2024-06-08T02:09:10.015101Z","iopub.status.idle":"2024-06-08T02:09:15.911841Z","shell.execute_reply.started":"2024-06-08T02:09:10.015076Z","shell.execute_reply":"2024-06-08T02:09:15.910896Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(42)  # For reproducibility\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"_uuid":"f00c216a-bd58-4a3f-a4a0-aee55e3e98b5","_cell_guid":"17f4a9bc-2f05-41f4-8199-98509d17cda4","collapsed":false,"id":"b17027ed","outputId":"2cad0f4a-27b0-4fb9-febb-6e50e7d468ee","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-08T02:09:15.913118Z","iopub.execute_input":"2024-06-08T02:09:15.914021Z","iopub.status.idle":"2024-06-08T02:09:15.923422Z","shell.execute_reply.started":"2024-06-08T02:09:15.913985Z","shell.execute_reply":"2024-06-08T02:09:15.922502Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"markdown","source":"### Multi-Head Attention\n\n$$\n    \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O \\\\\n    \\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) \\\\  \n    \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n$$","metadata":{"_uuid":"8eed7a1d-7e5b-409a-8633-1bd5066f9e02","_cell_guid":"c59e3ab5-f8ec-4419-8e49-3d24c2997035","id":"ffad93f9","trusted":true}},{"cell_type":"markdown","source":"Multi-head attention uses multiple attention mechanisms. Each head independently computes a set of attention weights (Wq, Wk, Wv) and multiplies it with Q, K, and V. The output of each head is later concatenated as the output of the multi-head attention. \n\nThe idea of multi-head attention is that each head is watching the full sentence but concentrates on different aspects of embeddings of each token.","metadata":{}},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n\n    \"\"\"The multi-head attention module\"\"\"\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n\n        # Ensure the dimension of the model is divisible by the number of heads.\n        # This is necessary to equally divide the embedding dimension across heads.\n        assert d_model % num_heads == 0, 'd_model must be divisible by num_heads'\n\n        self.d_model = d_model           # Total dimension of the model\n        self.num_heads = num_heads       # Number of attention heads\n        self.d_k = d_model // num_heads  # Dimnsion of each head. We assume d_v = d_k\n\n        # Linear transformations for queries, keys, and values\n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n\n        # Final linear layer to project the concatenated heads' outputs back to d_model dimensions\n        self.W_o = nn.Linear(d_model, d_model)\n\n    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n\n    # 1. Calculate attention scores with scaling.\n\n      scaled_dot_product = torch.matmul(Q,K.transpose(-2,-1))/math.sqrt(self.d_k)\n\n    # 2. Apply mask (if provided) by setting masked positions to a large negative value.\n\n      if mask is not None:\n        scaled_dot_product = scaled_dot_product.masked_fill(mask == 0, -1e9)\n\n    # 3. Apply softmax to attention scores to get probabilities.\n\n      attention_probabilities = torch.softmax(scaled_dot_product, dim = -1)\n\n    # 4. Return the weighted sum of values based on attention probabilities.\n\n      output = torch.matmul(attention_probabilities, V)\n\n      return output\n\n    def split_heads(self, x):\n        # Reshape the input tensor to [batch_size, num_heads, seq_length, d_k]\n        # to prepare for multi-head attention processing\n        batch_size, seq_length, d_model = x.size()\n        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n\n    def combine_heads(self, x):\n        # Inverse operation of split_heads: combine the head outputs back into the original tensor shape\n        # [batch_size, seq_length, d_model]\n        batch_size, num_heads, seq_length, d_k = x.size()\n        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n\n    def forward(self, Q, K, V, mask=None):\n\n\n      # 1. Linearly project the queries, keys, and values, and then split them into heads.\n\n      Q = self.split_heads(self.W_q(Q))\n      K = self.split_heads(self.W_q(K))\n      V = self.split_heads(self.W_q(V))\n\n      # 2. Apply scaled dot-product attention for each head.\n\n      attention_scores = self.scaled_dot_product_attention(Q, K, V, mask)\n\n      # 3. Concatenate the heads' outputs and apply the final linear projection.\n\n      output = self.W_o(self.combine_heads(attention_scores))\n      return output","metadata":{"_uuid":"db97d316-bd51-455f-97ad-1cc4c0e2df1d","_cell_guid":"a280762d-9946-44c0-8113-cff816752f23","collapsed":false,"id":"bf156177","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-08T02:09:15.924932Z","iopub.execute_input":"2024-06-08T02:09:15.925269Z","iopub.status.idle":"2024-06-08T02:09:15.937852Z","shell.execute_reply.started":"2024-06-08T02:09:15.925238Z","shell.execute_reply":"2024-06-08T02:09:15.936862Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Feed-Forward NN\n\n$$\n    \\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2\n$$","metadata":{"_uuid":"c158a387-f535-447a-be4b-4d30c0a46179","_cell_guid":"41a4d124-4b8d-400a-acde-3c6fda4a0cc6","id":"70b13867","trusted":true}},{"cell_type":"code","source":"class PositionwiseFeedForward(nn.Module):\n    \"\"\"The Positionwise Feedforward Network (FFN) module\"\"\"\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super().__init__()\n        self.linear1 = nn.Linear(d_model, d_ff)\n        self.linear2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n      x = self.linear1(x)\n      x = self.relu(x)\n      x = self.dropout(x)\n      x = self.linear2(x)\n      return x","metadata":{"_uuid":"bedd0714-96df-4e56-8c3c-64bc0ea98d5f","_cell_guid":"a4711a49-090b-4210-a4c7-08f81f96ebb1","collapsed":false,"id":"5d5ab551","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-08T02:09:15.939039Z","iopub.execute_input":"2024-06-08T02:09:15.939404Z","iopub.status.idle":"2024-06-08T02:09:15.948468Z","shell.execute_reply.started":"2024-06-08T02:09:15.939358Z","shell.execute_reply":"2024-06-08T02:09:15.947599Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### Positional Encoding\n\n$$\n    \\text{PE}(pos, 2i) = \\sin(pos/10000^{2i/d_{\\text{model}}}) \\\\\n    \\text{PE}(pos, 2i + 1) = \\cos(pos/10000^{2i/d_{\\text{model}}})\n$$","metadata":{"_uuid":"c31896ab-ddf0-4749-93f9-05c1a39a577a","_cell_guid":"5b6461b5-a7e8-4af6-9ef6-9b509124017a","id":"4034ef0a","trusted":true}},{"cell_type":"markdown","source":"Positional encodings help with retaining information about the position of each token. This is also one of the aspects that makes Transformers powerful allowing parallel computation unlike sequence of order like in RNNs or LSTMs. This incorporates the order of sequence.\n\nThese embediings are created using above formula of sinusoidal and cosine waves and are calcutaed only once.","metadata":{}},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    \"\"\"\n    Implements the positional encoding module using sinusoidal functions of different frequencies\n    for each dimension of the encoding.\n    \"\"\"\n    def __init__(self, d_model, max_seq_length):\n        super().__init__()\n\n        # Create a positional encoding (PE) matrix with dimensions [max_seq_length, d_model].\n        # This matrix will contain the positional encodings for all possible positions up to max_seq_length.\n        pe = torch.zeros(max_seq_length, d_model)\n\n        # Generate a tensor of positions (0 to max_seq_length - 1) and reshape it to [max_seq_length, 1].\n        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n\n        # Compute the division term used in the formulas for sin and cos functions.\n        # This term is based on the dimension of the model and the position, ensuring that the wavelengths\n        # form a geometric progression from 2π to 10000 * 2π. It uses only even indices for the dimensions.\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n\n        # Apply the sin function to even indices in the PE matrix. These values are determined by\n        # multiplying the position by the division term, creating a pattern where each position has\n        # a unique sinusoidal encoding.\n        pe[:, 0::2] = torch.sin(position * div_term)\n\n        # Apply the cos function to odd indices in the PE matrix, complementing the sin-encoded positions.\n        pe[:, 1::2] = torch.cos(position * div_term)\n\n        # Register 'pe' as a buffer within the module. Unlike parameters, buffers are not updated during training.\n        # This is crucial because positional encodings are fixed and not subject to training updates.\n        # The unsqueeze(0) adds a batch dimension for easier broadcasting with input tensors.\n        self.register_buffer('pe', pe.unsqueeze(0))\n\n    def forward(self, x):\n        # Add positional encoding to the input tensor x.\n        # x is expected to have dimensions [batch_size, seq_length, d_model].\n        # The positional encoding 'pe' is sliced to match the seq_length of 'x', and then added to 'x'.\n        # This operation leverages broadcasting to apply the same positional encoding across the batch.\n        x = x + self.pe[:, :x.size(1)]\n        return x","metadata":{"_uuid":"28c1511e-4afc-4da2-978c-6ac4a59343fb","_cell_guid":"ac8b7b69-434e-4859-8373-494759ef7cd4","collapsed":false,"id":"bfca835c","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-08T02:09:15.949839Z","iopub.execute_input":"2024-06-08T02:09:15.950799Z","iopub.status.idle":"2024-06-08T02:09:15.960765Z","shell.execute_reply.started":"2024-06-08T02:09:15.950773Z","shell.execute_reply":"2024-06-08T02:09:15.960025Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### Encoder Layer","metadata":{"_uuid":"db7a6ae9-780c-4b03-8e8c-a9b7b23da212","_cell_guid":"eb71aaa9-d8c3-4d60-b1cc-1763dac8f19d","id":"96e2475b","trusted":true}},{"cell_type":"markdown","source":"The encoder layer is the first layer in transformers architecture. It comprises of embeddings of input which are then positional encoded  followed by multi-headed attention layer, Feed forward network and normalisation layers.","metadata":{}},{"cell_type":"code","source":"class EncoderLayer(nn.Module):\n    \"\"\"An encoder layer consists of a multi-head self-attention sublayer and a feed forward sublayer,\n       with a dropout, residual connection, and layer normalization after each sub-layer.\n    \"\"\"\n    def __init__(self, d_model, num_heads, d_ff, dropout):\n        super().__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n        self.layer_norm1 = nn.LayerNorm(d_model)\n        self.layer_norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask):\n        attention_output = self.self_attn(x, x, x, mask)\n        x = self.layer_norm1(x + self.dropout(attention_output))\n        ff_output = self.feed_forward(x)\n        x = self.layer_norm2(x + self.dropout(ff_output))\n        return x","metadata":{"_uuid":"b9b80b1e-0a11-4bd1-9625-3893d64617a4","_cell_guid":"8627d166-bef8-4136-9b8d-cdac6597be2a","collapsed":false,"id":"000313c6","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-08T02:09:15.961841Z","iopub.execute_input":"2024-06-08T02:09:15.962557Z","iopub.status.idle":"2024-06-08T02:09:15.971232Z","shell.execute_reply.started":"2024-06-08T02:09:15.962525Z","shell.execute_reply":"2024-06-08T02:09:15.970467Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### Decoder Layer","metadata":{"_uuid":"b231a007-2a5b-43b8-baba-0a303867b1f2","_cell_guid":"688424f3-e477-440d-a336-66ac53f8afb2","id":"76215e02","trusted":true}},{"cell_type":"markdown","source":"\nThe decoder in the Transformer architecture is responsible for generating the output sequence (e.g., translated text) given the encoded input sequence from the encoder. The decoder is designed to predict the next token in the sequence by attending to both the previously generated tokens and the entire encoded input sequence.\n\nIt has a Masked Multi-Head Self-Attention (masked because decoder should not see future tokens, so its made causal). It also has cross attention layer - where it takes the keys and values calculated by encoder and takes the query from decoder.\n\n\n* Initialization: The decoder starts with an input sequence, often beginning with a start token (<bos>).\n* Self-Attention with Masking: For each position in the sequence, self-attention considers all previous positions but not future ones, thanks to the look-ahead mask.\n* Cross-Attention with Encoder Output: The decoder uses cross-attention to incorporate information from the encoder's output, effectively attending to the entire input sequence.\n* Generating the Next Token: The output of the final decoder layer is passed through a linear layer followed by a softmax function to predict the next token in the sequence.\n* Iterative Process: The predicted token is then appended to the input sequence, and the process repeats for the next position until an end-of-sequence token (<eos>) is generated or the maximum sequence length is reached.","metadata":{}},{"cell_type":"code","source":"class DecoderLayer(nn.Module):\n    \"\"\"A decoder layer consists of a multi-head self-attention, cross-attention and a feed-forward sublayers,\n       with a dropout, residual connection, and layer normalization after each sub-layer.\n    \"\"\"\n    def __init__(self, d_model, num_heads, d_ff, dropout):\n        super().__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n        self.layer_norm1 = nn.LayerNorm(d_model)\n        self.layer_norm2 = nn.LayerNorm(d_model)\n        self.layer_norm3 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, enc_output, src_mask, tgt_mask):\n      attention_output = self.self_attn(x, x, x, tgt_mask)\n      x = self.layer_norm1(x + self.dropout(attention_output))\n      attention_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n      x = self.layer_norm2(x + self.dropout(attention_output))\n      ff_output = self.feed_forward(x)\n      x = self.layer_norm3(x + self.dropout(ff_output))\n      return x","metadata":{"_uuid":"8df834b2-b379-42e2-8989-1131f3f7073e","_cell_guid":"393aee05-552b-4ce6-b97d-3e49c29bde91","collapsed":false,"id":"ee27007e","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-08T02:09:15.972215Z","iopub.execute_input":"2024-06-08T02:09:15.972466Z","iopub.status.idle":"2024-06-08T02:09:15.980702Z","shell.execute_reply.started":"2024-06-08T02:09:15.972444Z","shell.execute_reply":"2024-06-08T02:09:15.979922Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### The Full Model","metadata":{"_uuid":"d02c5f81-8d63-4e03-9f26-33e5b6de91ce","_cell_guid":"a273a682-d139-4ced-818c-38ae965529e4","id":"2ad86614","trusted":true}},{"cell_type":"code","source":"class Transformer(nn.Module):\n    \"\"\"\n    Implements the Transformer model for sequence-to-sequence tasks such as machine translation.\n    The Transformer model, as described in \"Attention is All You Need\" by Vaswani et al., consists of an encoder and\n    decoder architecture that uses self-attention mechanisms to process input sequences and generate output sequences.\n\n    Parameters:\n    - src_vocab_size (int): Size of the source vocabulary.\n    - tgt_vocab_size (int): Size of the target vocabulary.\n    - d_model (int): Dimension of the model embeddings and hidden states.\n    - N (int): Number of layers in both the encoder and decoder stacks.\n    - n_heads (int): Number of attention heads in each multi-head attention mechanism.\n    - d_ff (int): Dimension of the feed-forward network within each layer.\n    - max_seq_length (int): Maximum length of input sequences, used for positional encoding.\n    - dropout (float): Dropout rate applied to embeddings and sub-layers.\n    - pad_idx (int): Index of the padding token in the source and target vocabularies.\n\n    Attributes:\n    - src_embedding (torch.nn.Embedding): Embedding layer for source sequences.\n    - tgt_embedding (torch.nn.Embedding): Embedding layer for target sequences.\n    - positional_encoding (PositionalEncoding): Adds positional information to embeddings.\n    - encoder (torch.nn.ModuleList): Stack of N encoder layers.\n    - decoder (torch.nn.ModuleList): Stack of N decoder layers.\n    - out (torch.nn.Linear): Linear layer that projects decoder output to target vocabulary size.\n    - dropout (torch.nn.Dropout): Dropout layer applied after embedding and positional encoding.\n\n    Methods:\n    - init_weights: Initializes model parameters using Glorot uniform initialization.\n    - create_source_mask: Creates a mask for padding tokens in the source sequence to ignore them in attention computations.\n    - create_target_mask: Creates combined padding and future token masks for the target sequence to prevent attending to future tokens and padding tokens.\n    - encode: Processes the source sequence through the encoder stack and generates memory states.\n    - decode: Processes the target sequence through the decoder stack using memory states from the encoder and applicable masks.\n    - forward: Defines the forward pass of the model using the encode and decode methods.\n    \"\"\"\n    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, N, n_heads, d_ff, max_seq_length, dropout, pad_idx):\n        super().__init__()\n\n        # Embedding layers for source and target\n        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n\n        # Positional encoding\n        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n\n        # Encoder and Decoder stacks\n        self.encoder = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(N)])\n        self.decoder = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(N)])\n\n        # Output linear layer\n        self.out = nn.Linear(d_model, tgt_vocab_size)\n\n        self.dropout = nn.Dropout(dropout)\n\n        # Initialization\n        self.init_weights()\n        self.pad_idx = pad_idx\n\n    def init_weights(self):\n        \"\"\"Initialize parameters with Glorot / fan_avg\"\"\"\n        for p in self.parameters():\n            if p.dim() > 1:\n                nn.init.xavier_uniform_(p)\n\n    def create_source_mask(self, src):\n        \"\"\"Create a mask for padding tokens in the source\"\"\"\n        src_mask = (src != self.pad_idx).unsqueeze(1).unsqueeze(2)  # [batch_size, 1, 1, src_len]\n        # unsqueeze(1) adds a dimension for the heads of the multi-head attention\n        # unsqueeze(2) adds a dimension for the attention scores\n        # This mask can be broadcasted across the src_len dimension of the attention scores,\n        # effectively masking out specific tokens across all heads and all positions in the sequence.\n        return src_mask\n\n    def create_target_mask(self, tgt):\n        \"\"\"Create masks for both padding tokens and future tokens\"\"\"\n        # Target padding mask\n        tgt_pad_mask = (tgt != self.pad_idx).unsqueeze(1).unsqueeze(3)  # [batch_size, 1, tgt_len, 1]\n        # unsqueeze(1) adds a dimension for the heads of the multi-head attention\n        # unsqueeze(3) adds a dimension for the attention scores\n        # The final shape allows the mask to be broadcast across the attention scores, ensuring positions only\n        # attend to allowed positions as dictated by the no-peak mask (the preceding positions) and the padding mask.\n\n        # Target no-peak mask\n        tgt_len = tgt.size(1)\n        tgt_nopeak_mask = torch.tril(torch.ones(tgt_len, tgt_len, device=device)).bool()\n\n        # Combine masks\n        tgt_mask = tgt_pad_mask & tgt_nopeak_mask  # [batch_size, 1, tgt_len, tgt_len]\n        return tgt_mask\n\n    def encode(self, src):\n        \"\"\"Encodes the source sequence using the Transformer encoder stack.\n        \"\"\"\n        src_mask = self.create_source_mask(src)\n        src = self.dropout(self.positional_encoding(self.src_embedding(src)))\n\n        # Pass through each layer in the encoder\n        for layer in self.encoder:\n            src = layer(src, src_mask)\n        return src, src_mask\n\n    def decode(self, tgt, memory, src_mask):\n        \"\"\"Decodes the target sequence using the Transformer decoder stack, given the memory from the encoder.\n        \"\"\"\n        tgt_mask = self.create_target_mask(tgt)\n        tgt = self.dropout(self.positional_encoding(self.tgt_embedding(tgt)))\n\n        # Pass through each layer in the decoder\n        for layer in self.decoder:\n            tgt = layer(tgt, memory, src_mask, tgt_mask)\n\n        # Output layer\n        output = self.out(tgt)\n        return output\n\n    def forward(self, src, tgt):\n        src_mask = self.create_source_mask(src)\n        tgt_mask = self.create_target_mask(tgt)\n        src_embedded = self.dropout(self.positional_encoding(self.src_embedding(src)))\n        tgt_embedded = self.dropout(self.positional_encoding(self.tgt_embedding(tgt)))\n\n        enc_output = src_embedded\n        for enc_layer in self.encoder:\n            enc_output = enc_layer(enc_output, src_mask)\n\n        dec_output = tgt_embedded\n        for dec_layer in self.decoder:\n            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n\n        output = self.out(dec_output)\n\n        return output","metadata":{"_uuid":"d86a1a07-4247-4ae5-95f3-c62551a630d2","_cell_guid":"41e3920c-7758-41d6-b306-d1ffca9e96a6","collapsed":false,"id":"31af1f2a","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-08T02:09:15.984841Z","iopub.execute_input":"2024-06-08T02:09:15.985128Z","iopub.status.idle":"2024-06-08T02:09:16.005102Z","shell.execute_reply.started":"2024-06-08T02:09:15.985106Z","shell.execute_reply":"2024-06-08T02:09:16.004222Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Define the hyperparameters of the model\nsrc_vocab_size = 5000  # Size of source vocabulary\ntgt_vocab_size = 5000  # Size of target vocabulary\nd_model = 512          # Embedding dimension\nN = 6                  # Number of encoder and decoder layers\nnum_heads = 8          # Number of attention heads\nd_ff = 2048            # Dimension of feed forward networks\nmax_seq_length = 100   # Maximum sequence length\ndropout = 0.1          # Dropout rate\npad_idx = 0            # Index of the padding token\n\nmodel = Transformer(src_vocab_size, tgt_vocab_size, d_model, N, num_heads, d_ff, max_seq_length, dropout, pad_idx)\n\n# Move the model to the appropriate device (GPU or CPU)\nmodel = model.to(device)","metadata":{"_uuid":"5bcd51d7-2ef9-47fc-b058-c309449977b8","_cell_guid":"e809e6e5-1f4d-45ef-989b-4548602b609e","collapsed":false,"id":"11a3b60d","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-08T02:09:16.006540Z","iopub.execute_input":"2024-06-08T02:09:16.007038Z","iopub.status.idle":"2024-06-08T02:09:17.264640Z","shell.execute_reply.started":"2024-06-08T02:09:16.007007Z","shell.execute_reply":"2024-06-08T02:09:17.263842Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"### Testing on Random Data","metadata":{"_uuid":"a923fbfd-a081-4133-8ef5-13695c25b1b2","_cell_guid":"8664bdde-c5da-4a81-9faa-bf18c9971e5b","id":"050f7248","trusted":true}},{"cell_type":"code","source":"# Generate random sample data\ntorch.manual_seed(42)\n\nsrc_data = torch.randint(1, src_vocab_size, (64, max_seq_length)).to(device)  # (batch_size, seq_length)\ntgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length)).to(device)  # (batch_size, seq_length)","metadata":{"_uuid":"dac53993-1f90-498f-b967-9540162535b1","_cell_guid":"1499c311-81de-48ce-b8b7-c71da691b530","collapsed":false,"id":"149dadd1","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-08T02:09:17.265929Z","iopub.execute_input":"2024-06-08T02:09:17.266600Z","iopub.status.idle":"2024-06-08T02:09:17.274293Z","shell.execute_reply.started":"2024-06-08T02:09:17.266565Z","shell.execute_reply":"2024-06-08T02:09:17.273588Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"#### Inference","metadata":{"_uuid":"2082ec9e-5d15-4c1a-b225-46b0653dc6e2","_cell_guid":"145c3e0f-0149-4b59-b7bf-96e747a4ef9d","id":"2d5b1c56","trusted":true}},{"cell_type":"code","source":"# Generate the next token using the first token in the first target tensor\nmodel.eval()\n\nmemory, src_mask = model.encode(src_data[:1, :])\noutput = model.decode(tgt_data[:1, :1], memory, src_mask)\ny = output.view(-1, tgt_vocab_size).argmax(-1)\ny","metadata":{"_uuid":"620942f6-cd2d-4ae8-9de2-88dcc93c507d","_cell_guid":"7dc8052c-d551-4eed-95d3-664140308fa7","collapsed":false,"id":"45583975","outputId":"1830859d-9f5e-4a88-b679-486698b5f1cd","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-08T02:09:17.275499Z","iopub.execute_input":"2024-06-08T02:09:17.275781Z","iopub.status.idle":"2024-06-08T02:09:18.112464Z","shell.execute_reply.started":"2024-06-08T02:09:17.275756Z","shell.execute_reply":"2024-06-08T02:09:18.111536Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"tensor([1107], device='cuda:0')"},"metadata":{}}]},{"cell_type":"markdown","source":"If your code is correct, you should get tensor([990]).","metadata":{"_uuid":"0723984e-8940-43ed-9a0c-55e4de2ef96d","_cell_guid":"845e0efc-7ff6-4a4e-a489-e0fa9316682c","id":"18b7e1c5","trusted":true}},{"cell_type":"markdown","source":"#### Training","metadata":{"_uuid":"61edd15f-e08a-469f-ae03-6d190aa5c781","_cell_guid":"808ab286-2a52-4507-bcb0-779fdedbdb8b","id":"a314148e","trusted":true}},{"cell_type":"code","source":"# Train the model for 10 epochs\ncriterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\noptimizer = optim.Adam(model.parameters(), lr=0.0005, betas=(0.9, 0.98), eps=1e-9)\ngrad_clip = 1\nn_epochs = 10\n\nmodel.train()\n\nfor epoch in range(n_epochs):\n    optimizer.zero_grad()\n\n    # Forward pass\n    output = model(src_data, tgt_data[:, :-1])\n\n    # tgt_data is of shape [batch_size, tgt_len]\n    # output is of shape [batch_size, tgt_len, tgt_vocab_size]\n    output = output.contiguous().view(-1, tgt_vocab_size)\n    tgt = tgt_data[:, 1:].contiguous().view(-1)\n    loss = criterion(output, tgt)\n\n    loss.backward()\n    nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n    optimizer.step()\n    print(f'Epoch: {epoch + 1}, Loss: {loss.item()}')","metadata":{"_uuid":"15ef43a5-3209-49a0-b625-e4e0648eb385","_cell_guid":"ed0e44f2-ad68-41db-ae47-77837fec4db2","collapsed":false,"id":"2afd8ff3","outputId":"07dc3f65-9241-4d9a-ff96-0ea2217193bb","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-08T02:09:18.113830Z","iopub.execute_input":"2024-06-08T02:09:18.114469Z","iopub.status.idle":"2024-06-08T02:09:24.477299Z","shell.execute_reply.started":"2024-06-08T02:09:18.114434Z","shell.execute_reply":"2024-06-08T02:09:24.476327Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Epoch: 1, Loss: 8.600719451904297\nEpoch: 2, Loss: 8.511482238769531\nEpoch: 3, Loss: 8.380781173706055\nEpoch: 4, Loss: 8.304323196411133\nEpoch: 5, Loss: 8.243993759155273\nEpoch: 6, Loss: 8.20079231262207\nEpoch: 7, Loss: 8.170716285705566\nEpoch: 8, Loss: 8.145832061767578\nEpoch: 9, Loss: 8.135025024414062\nEpoch: 10, Loss: 8.123465538024902\n","output_type":"stream"}]},{"cell_type":"markdown","source":"You should see the loss decreasing from around 8.6 to 8.1.","metadata":{"_uuid":"85916302-31d1-4ecd-aac0-ba0311767666","_cell_guid":"632a2ea4-c2a3-4d18-a061-d83ebe577e4d","id":"42f36f48","trusted":true}},{"cell_type":"markdown","source":"### Machine Translation Example\n\nWe now consider a real-world example using the Multi30k German-English translation task. This task is much smaller than the WMT task considered in the paper (only 30K sentence pairs compared to 4.5M pairs in the WMT-14 English-German dataset), but it illustrates the whole system. <br>\nIt is recommended to run this example on Google Colab, or on a machine with a strong GPU.","metadata":{"_uuid":"427a23c7-1991-40b0-b1a8-572c50b422da","_cell_guid":"31e32a49-41fd-477b-80ee-7df77518498e","id":"71caa7ed","trusted":true}},{"cell_type":"markdown","source":"#### Define Tokenizers","metadata":{"_uuid":"9d2c7584-46bc-4084-abc1-31cb5ffd638b","_cell_guid":"95516605-af5c-4e8c-8a44-b5a6d5863765","id":"3d6ecd8c","trusted":true}},{"cell_type":"code","source":"# Load spacy models for tokenization\ntry:\n    spacy_de = spacy.load('de_core_news_sm')\nexcept IOError:\n    os.system(\"python -m spacy download de_core_news_sm\")\n    spacy_de = spacy.load('de_core_news_sm')\n\ntry:\n    spacy_en = spacy.load('en_core_web_sm')\nexcept IOError:\n    os.system(\"python -m spacy download en_core_web_sm\")\n    spacy_en = spacy.load('en_core_web_sm')\n\ndef tokenize_de(text):\n    return [tok.text for tok in spacy_de.tokenizer(text)]\n\ndef tokenize_en(text):\n    return [tok.text for tok in spacy_en.tokenizer(text)]\n\ndef yield_tokens(data_iter, tokenizer, language):\n    for data_sample in data_iter:\n        yield tokenizer(data_sample[language])\n\ntokenizer_de = get_tokenizer(tokenize_de)\ntokenizer_en = get_tokenizer(tokenize_en)","metadata":{"_uuid":"8cc5bacd-05b7-4afd-8c0a-9f8682499381","_cell_guid":"084ddf82-4b53-4f36-9e29-9fe07f518e21","collapsed":false,"id":"08f20abd","outputId":"640f1e86-4cbc-41c0-bdb5-26edb504e438","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-08T02:09:24.478735Z","iopub.execute_input":"2024-06-08T02:09:24.479280Z","iopub.status.idle":"2024-06-08T02:09:44.353488Z","shell.execute_reply.started":"2024-06-08T02:09:24.479253Z","shell.execute_reply":"2024-06-08T02:09:44.352675Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Collecting de-core-news-sm==3.7.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.7.0/de_core_news_sm-3.7.0-py3-none-any.whl (14.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /opt/conda/lib/python3.10/site-packages (from de-core-news-sm==3.7.0) (3.7.3)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.10)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.8)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.9)\nRequirement already satisfied: thinc<8.3.0,>=8.2.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.2.3)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.4.8)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.10)\nRequirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.3.4)\nRequirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.9.0)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (6.4.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.66.4)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.32.3)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.5.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.1.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (69.0.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (21.3)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.4.0)\nRequirement already satisfied: numpy>=1.19.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.26.4)\nRequirement already satisfied: language-data>=1.2 in /opt/conda/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.2.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.1.1)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.14.6)\nRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2024.2.2)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.10)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.4)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.1.7)\nRequirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.1.3)\nRequirement already satisfied: marisa-trie>=0.7.7 in /opt/conda/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.1.0)\nInstalling collected packages: de-core-news-sm\nSuccessfully installed de-core-news-sm-3.7.0\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('de_core_news_sm')\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Build Vocabularies","metadata":{"_uuid":"25def68b-1e82-4ab8-9ac8-94f5fe5ac488","_cell_guid":"c264dbd9-a3e7-42df-b515-b32785315c6c","id":"e909ce2a","trusted":true}},{"cell_type":"code","source":"train_data, _, _ = Multi30k(split=('train', 'valid', 'test'))\nvocab_src = build_vocab_from_iterator(yield_tokens(train_data, tokenizer_de, 0),\n                                      specials=['<unk>', '<pad>', '<bos>', '<eos>'])\nvocab_tgt = build_vocab_from_iterator(yield_tokens(train_data, tokenizer_en, 1),\n                                      specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n\nvocab_src.set_default_index(vocab_src['<unk>'])\nvocab_tgt.set_default_index(vocab_tgt['<unk>'])","metadata":{"_uuid":"8dd5532e-ef49-4a66-a8c2-ad35334c0eaa","_cell_guid":"d9b897d2-e330-4fe2-bffb-edff9c93f546","collapsed":false,"id":"aed8dff9","outputId":"6b3db6c2-077f-4b3b-87ea-d0503a991859","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-08T02:09:44.354607Z","iopub.execute_input":"2024-06-08T02:09:44.354917Z","iopub.status.idle":"2024-06-08T02:09:50.812259Z","shell.execute_reply.started":"2024-06-08T02:09:44.354892Z","shell.execute_reply":"2024-06-08T02:09:50.811456Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/datapipes/iter/combining.py:333: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Create the Transformer","metadata":{"_uuid":"19790a9d-1288-4326-add0-19d1ef105421","_cell_guid":"4ecc2975-292f-42ca-b1b7-c958df98c981","id":"936560a1","trusted":true}},{"cell_type":"code","source":"# Define the hyperparameters of the model\nsrc_vocab_size = len(vocab_src)  # Size of source vocabulary\ntgt_vocab_size = len(vocab_tgt)  # Size of target vocabulary\nd_model = 512  # Embedding dimension\nN = 6          # Number of encoder and decoder layers\nnum_heads = 8  # Number of attention heads\nd_ff = 2048    # Dimension of feed forward networks\nmax_seq_length = 5000 # Maximum sequence length\ndropout = 0.1  # Dropout rate\n\n# Assume pad_idx is the padding index in the target vocabulary\npad_idx = vocab_tgt['<pad>']\n\n# Initialize the Transformer model\nmodel = Transformer(src_vocab_size, tgt_vocab_size, d_model, N, num_heads, d_ff, max_seq_length, dropout, pad_idx)\n\n# Move the model to the appropriate device (GPU or CPU)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\n\n# Hyperparameters for the training process\nbatch_size = 128\ngrad_clip = 1\noptimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n\n# Initialize the loss function with CrossEntropyLoss, ignoring the padding index\ncriterion = nn.CrossEntropyLoss(ignore_index=pad_idx)","metadata":{"_uuid":"b0a85e72-c6eb-43e4-a8aa-d844445cdd2e","_cell_guid":"13a1bfea-4a8a-4a68-b8d6-0ed34d15504a","collapsed":false,"id":"4c862170","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-08T02:09:50.813496Z","iopub.execute_input":"2024-06-08T02:09:50.813856Z","iopub.status.idle":"2024-06-08T02:09:52.048988Z","shell.execute_reply.started":"2024-06-08T02:09:50.813831Z","shell.execute_reply":"2024-06-08T02:09:52.048209Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"#### Data Processing","metadata":{"_uuid":"73c2c2f7-9f3d-4d5f-86db-b5552091bb48","_cell_guid":"89281099-b207-4da6-869e-9ab1bedf1f74","id":"b3d3b94d","trusted":true}},{"cell_type":"code","source":"def data_process(raw_data_iter):\n    data = []\n    for raw_src, raw_tgt in raw_data_iter:\n        src_tensor = torch.tensor([vocab_src[token] for token in tokenizer_de(raw_src)], dtype=torch.long)\n        tgt_tensor = torch.tensor([vocab_tgt[token] for token in tokenizer_en(raw_tgt)], dtype=torch.long)\n        data.append((src_tensor, tgt_tensor))\n    return data\n\ntrain_data, valid_data, test_data = Multi30k(split=('train', 'valid', 'test'))\ntrain_data = data_process(train_data)\nvalid_data = data_process(valid_data)\n#test_data = data_process(test_data)\n# The test set of Multi30k is corrupted\n# See https://discuss.pytorch.org/t/unicodedecodeerror-when-running-test-iterator/192818/3","metadata":{"_uuid":"a9bb4fd6-255b-4c0c-962a-ec27e3e20ca6","_cell_guid":"e67cc670-90d9-4d00-80a2-914a91cd38fb","collapsed":false,"id":"3ca10d94","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-08T02:09:52.050135Z","iopub.execute_input":"2024-06-08T02:09:52.050488Z","iopub.status.idle":"2024-06-08T02:09:56.725646Z","shell.execute_reply.started":"2024-06-08T02:09:52.050462Z","shell.execute_reply":"2024-06-08T02:09:56.724830Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def generate_batch(data_batch):\n    \"\"\"Processes a batch of source-target pairs by adding start-of-sequence (BOS) and end-of-sequence (EOS) tokens\n    to each sequence and padding all sequences to the same length.\n\n    Parameters:\n    - data_batch (Iterable[Tuple[Tensor, Tensor]]): A batch of source-target pairs, where each element is a tuple\n      containing the source sequence tensor and the target sequence tensor.\n    \"\"\"\n    src_batch, tgt_batch = [], []\n    src_batch, tgt_batch = [], []\n\n    # Iterate over each source-target pair in the provided batch\n    for src_item, tgt_item in data_batch:\n        # Prepend the start-of-sequence (BOS) token and append the end-of-sequence (EOS) token to the sequences\n        src_batch.append(torch.cat([torch.tensor([vocab_src['<bos>']]), src_item,\n                                    torch.tensor([vocab_src['<eos>']])], dim=0))\n        tgt_batch.append(torch.cat([torch.tensor([vocab_tgt['<bos>']]), tgt_item,\n                                    torch.tensor([vocab_tgt['<eos>']])], dim=0))\n\n    # Pad the sequences in the source batch to ensure they all have the same length.\n    # 'batch_first=True' indicates that the batch dimension should come first in the resulting tensor.\n    src_batch = pad_sequence(src_batch, padding_value=vocab_src['<pad>'], batch_first=True)\n    tgt_batch = pad_sequence(tgt_batch, padding_value=vocab_tgt['<pad>'], batch_first=True)\n    return src_batch, tgt_batch\n\n# DataLoader for the training data, using the generate_batch function as the collate_fn.\n# This allows custom processing of each batch (adding BOS/EOS tokens and padding) before being fed into the model.\ntrain_iterator = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=generate_batch)\n\n# Similarly, DataLoader for the validation data\nvalid_iterator = DataLoader(valid_data, batch_size=batch_size, shuffle=True, collate_fn=generate_batch)","metadata":{"_uuid":"7ee8a861-27e0-4d48-9515-db661455d9c3","_cell_guid":"dd06973e-0332-4c5a-ae10-6d423311e1d1","collapsed":false,"id":"1c2f452a","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-08T02:09:56.726664Z","iopub.execute_input":"2024-06-08T02:09:56.726932Z","iopub.status.idle":"2024-06-08T02:09:56.735924Z","shell.execute_reply.started":"2024-06-08T02:09:56.726908Z","shell.execute_reply":"2024-06-08T02:09:56.734952Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"def train(model, iterator, optimizer, criterion, grad_clip):\n    \"\"\"\n    Trains the model for one epoch over the given dataset.\n    This function iterates over the provided data iterator, performing the forward and backward passes for each batch.\n    It employs teacher forcing by feeding the shifted target sequence (excluding the last token) as input to the decoder.\n\n    Parameters:\n    - model (torch.nn.Module): The model to be trained.\n    - iterator (Iterable): An iterable object that returns batches of data.\n    - optimizer (torch.optim.Optimizer): The optimizer to use for updating the model parameters.\n    - criterion (Callable): The loss function used to compute the difference between the model's predictions and the actual targets.\n    - grad_clip (float): The maximum norm of the gradients for gradient clipping.\n\n    Returns:\n    - float: The average loss for the epoch, computed as the total loss over all batches divided by the number of batches in the iterator.\n    \"\"\"\n    # Set the model to training mode.\n    # This enables dropout, layer normalization etc., which behave differently during training.\n    model.train()\n\n    epoch_loss = 0\n\n    # Enumerate over the data iterator to get batches\n    for i, batch in enumerate(iterator):\n        # Unpack the batch to get source (src) and target (tgt) sequences\n        src, tgt = batch\n        src, tgt = src.to(device), tgt.to(device)\n        optimizer.zero_grad()\n\n        # Forward pass through the model.\n        # For seq2seq models, the decoder input (tgt[:, :-1]) excludes the last token, implementing teacher forcing.\n        output = model(src, tgt[:, :-1])\n\n        # Reshape the output and target tensors to compute loss.\n        # The output tensor is reshaped to a 2D tensor where rows correspond to each token in the batch and columns to vocabulary size.\n\n        # tgt is of shape [batch_size, tgt_len]\n        # output is of shape [batch_size, tgt_len, tgt_vocab_size]\n        output = output.contiguous().view(-1, tgt_vocab_size)\n\n        # The target tensor is reshaped to a 1D tensor, excluding the first token (BOS) from each sequence.\n        tgt = tgt[:, 1:].contiguous().view(-1)\n\n        # Compute loss, perform backpropagation, and update model parameters\n        loss = criterion(output, tgt)\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n        optimizer.step()\n        epoch_loss += loss.item()\n\n    # Compute average loss per batch for the current epoch\n    return epoch_loss / len(iterator)","metadata":{"_uuid":"a9f40f60-7251-4d75-b389-c927392d6a2e","_cell_guid":"cc0675c2-ca68-49e3-bf0c-6deb5fd4be9a","collapsed":false,"id":"2cbd8431","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-08T02:09:56.737139Z","iopub.execute_input":"2024-06-08T02:09:56.737424Z","iopub.status.idle":"2024-06-08T02:09:56.749637Z","shell.execute_reply.started":"2024-06-08T02:09:56.737399Z","shell.execute_reply":"2024-06-08T02:09:56.748805Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, iterator, criterion):\n    \"\"\"\n    Evaluates the model's performance on a given dataset.\n    This function is similar to the training loop, but without the backward pass and parameter updates.\n    \"\"\"\n    model.eval()\n    epoch_loss = 0\n\n    with torch.no_grad():\n        for i, batch in enumerate(iterator):\n            src, tgt = batch\n            src, tgt = src.to(device), tgt.to(device)\n            output = model(src, tgt[:, :-1])\n            output_dim = output.shape[-1]\n            output = output.contiguous().view(-1, output_dim)\n            tgt = tgt[:, 1:].contiguous().view(-1)\n            loss = criterion(output, tgt)\n            epoch_loss += loss.item()\n\n    return epoch_loss / len(iterator)","metadata":{"_uuid":"f71ae0f4-432c-48fd-b5e3-9a40d78ca187","_cell_guid":"76aaa2f5-1fd4-47e6-891d-d4b04e83df76","collapsed":false,"id":"31d18844","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-08T02:09:56.750776Z","iopub.execute_input":"2024-06-08T02:09:56.751445Z","iopub.status.idle":"2024-06-08T02:09:56.760465Z","shell.execute_reply.started":"2024-06-08T02:09:56.751416Z","shell.execute_reply":"2024-06-08T02:09:56.759668Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"#### Training the Model","metadata":{"_uuid":"a08a20e7-cab4-411a-b678-42f2853df593","_cell_guid":"abf4ce13-e74b-4724-b270-a2793f995370","id":"7ba53f24","trusted":true}},{"cell_type":"code","source":"n_epochs = 20\n\nfor epoch in range(n_epochs):\n    train_loss = train(model, train_iterator, optimizer, criterion, grad_clip)\n    val_loss = evaluate(model, valid_iterator, criterion)\n\n    print(f'\\nEpoch: {epoch + 1}')\n    print(f'\\tTrain Loss: {train_loss:.3f}')\n    print(f'\\tVal Loss: {val_loss:.3f}')","metadata":{"_uuid":"75fd2889-b3be-4a91-8e6a-16c77a9188e7","_cell_guid":"50dc2150-1536-434f-a4dc-0c19cc2c9e61","collapsed":false,"id":"d02463b8","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-08T02:09:56.761460Z","iopub.execute_input":"2024-06-08T02:09:56.761752Z","iopub.status.idle":"2024-06-08T02:38:21.968511Z","shell.execute_reply.started":"2024-06-08T02:09:56.761717Z","shell.execute_reply":"2024-06-08T02:38:21.967292Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"\nEpoch: 1\n\tTrain Loss: 5.667\n\tVal Loss: 5.009\n\nEpoch: 2\n\tTrain Loss: 4.854\n\tVal Loss: 4.846\n\nEpoch: 3\n\tTrain Loss: 4.649\n\tVal Loss: 4.571\n\nEpoch: 4\n\tTrain Loss: 4.310\n\tVal Loss: 4.254\n\nEpoch: 5\n\tTrain Loss: 4.034\n\tVal Loss: 3.997\n\nEpoch: 6\n\tTrain Loss: 3.844\n\tVal Loss: 3.898\n\nEpoch: 7\n\tTrain Loss: 3.705\n\tVal Loss: 3.772\n\nEpoch: 8\n\tTrain Loss: 3.597\n\tVal Loss: 3.671\n\nEpoch: 9\n\tTrain Loss: 3.505\n\tVal Loss: 3.652\n\nEpoch: 10\n\tTrain Loss: 3.420\n\tVal Loss: 3.540\n\nEpoch: 11\n\tTrain Loss: 3.343\n\tVal Loss: 3.497\n\nEpoch: 12\n\tTrain Loss: 3.271\n\tVal Loss: 3.452\n\nEpoch: 13\n\tTrain Loss: 3.200\n\tVal Loss: 3.383\n\nEpoch: 14\n\tTrain Loss: 3.138\n\tVal Loss: 3.346\n\nEpoch: 15\n\tTrain Loss: 3.075\n\tVal Loss: 3.301\n\nEpoch: 16\n\tTrain Loss: 3.017\n\tVal Loss: 3.230\n\nEpoch: 17\n\tTrain Loss: 2.962\n\tVal Loss: 3.204\n\nEpoch: 18\n\tTrain Loss: 2.907\n\tVal Loss: 3.168\n\nEpoch: 19\n\tTrain Loss: 2.854\n\tVal Loss: 3.117\n\nEpoch: 20\n\tTrain Loss: 2.802\n\tVal Loss: 3.107\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The train loss should decrease from around 5.7 to 2.8 after 20 epochs.","metadata":{"_uuid":"6f2a25b3-17bb-4d2c-83a6-73aa4ab741ab","_cell_guid":"8eab6145-4955-4234-a24b-0193d6d28b5c","id":"f7f39e45","trusted":true}},{"cell_type":"markdown","source":"#### Translating a Sample Sentence","metadata":{"_uuid":"57e53c22-2760-4c5e-947b-f6c9687fd5ec","_cell_guid":"cea962db-c040-476e-97a7-eae5cc659314","id":"fc7d30f7","trusted":true}},{"cell_type":"code","source":"def translate_sentence(model, sentence, vocab_src, vocab_tgt, max_length=50, device='cuda'):\n    \"\"\"\n    Translates a given source sentence into the target language using a trained Transformer model.\n    The function preprocesses the input sentence by tokenizing and converting it to tensor format, then uses the model's\n    encode and decode methods to generate the translated sentence. The translation process is performed token by token\n    using greedy decoding, selecting the most likely next token at each step until an <eos> token is produced or the\n    maximum length is reached.\n\n    Parameters:\n    - model (torch.nn.Module): The trained Transformer model.\n    - sentence (str): The source sentence to translate.\n    - vocab_src (Vocab): The source vocabulary mapping of tokens to indices. It should include special tokens such as\n      '<bos>' (beginning of sentence) and '<eos>' (end of sentence).\n    - vocab_tgt (Vocab): The target vocabulary mapping of indices to tokens. It should provide a method `lookup_token`\n      to convert token indices back to the string representation.\n    - max_length (int, optional): The maximum allowed length for the generated translation. The decoding process will\n      stop when this length is reached if an <eos> token has not yet been generated.\n\n    Returns:\n    - str: The translated sentence as a string of text in the target language.\n    \"\"\"\n    model.eval()\n    device = torch.device(device if torch.cuda.is_available() else 'cpu')\n\n    # Tokenize and convert the source sentence to tensor format\n    tokens = tokenizer_de(sentence)  # Use the tokenizer defined for German\n    src_indices = [vocab_src['<bos>']] + [vocab_src[token] for token in tokens if token in vocab_src] + [vocab_src['<eos>']]\n    src_tensor = torch.LongTensor(src_indices).unsqueeze(0).to(device)  # Add batch dimension and move to device\n\n    # Encode the source sentence\n    memory, src_mask = model.encode(src_tensor)\n\n    # Initialize target sequence with <bos> token\n    tgt_indices = [vocab_tgt['<bos>']]\n    tgt_tensor = torch.LongTensor(tgt_indices).unsqueeze(0).to(device)\n\n    for _ in range(max_length):\n        # Decode the target sequence using the memory from the encoder\n        output = model.decode(tgt_tensor, memory, src_mask)\n\n        # Get the next token\n        logits = output[:, -1, :]  # Get the logits for the last time step\n        prob = F.softmax(logits, dim=-1)\n        next_token = torch.argmax(prob, dim=-1).item()\n\n        # Append the predicted token to the target sequence\n        tgt_indices.append(next_token)\n        tgt_tensor = torch.LongTensor(tgt_indices).unsqueeze(0).to(device)\n\n        # Break if end of sentence token is produced\n        if next_token == vocab_tgt['<eos>']:\n            break\n\n    # Convert token indices back to string representation\n    translated_tokens = [vocab_tgt.lookup_token(idx) for idx in tgt_indices]\n    translated_sentence = ' '.join(translated_tokens[1:])  # Remove the initial <bos> token\n\n    return translated_sentence","metadata":{"_uuid":"09f59cfd-ad13-4ebc-b1b1-f239f2b1fb41","_cell_guid":"f143a324-c7b4-49f9-aaee-6a4d36df87d3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-08T02:38:21.972029Z","iopub.execute_input":"2024-06-08T02:38:21.972336Z","iopub.status.idle":"2024-06-08T02:38:21.985183Z","shell.execute_reply.started":"2024-06-08T02:38:21.972311Z","shell.execute_reply":"2024-06-08T02:38:21.984228Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"import torch.nn.functional as F","metadata":{"_uuid":"ceb04cde-a74e-44ea-be3c-db0e2787bf08","_cell_guid":"b8081ac3-f27e-42d2-9b73-ca97b906a86c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-08T02:38:21.986417Z","iopub.execute_input":"2024-06-08T02:38:21.986758Z","iopub.status.idle":"2024-06-08T02:38:22.015366Z","shell.execute_reply.started":"2024-06-08T02:38:21.986726Z","shell.execute_reply":"2024-06-08T02:38:22.014401Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\nsrc_sentence = \"Ein kleiner Junge spielt draußen mit einem Ball.\"  # German for \"A little boy playing outside with a ball.\"\ntranslated_sentence = translate_sentence(model, src_sentence, vocab_src, vocab_tgt,device = device)\nprint(f'Translated sentence: {translated_sentence}')","metadata":{"_uuid":"bd2d3c73-2f6b-4de1-b1fe-f160d74992b6","_cell_guid":"b8cf9f4b-1505-45f8-86fb-f6fe7516bfc4","collapsed":false,"id":"baeb62e4","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-08T02:38:22.016570Z","iopub.execute_input":"2024-06-08T02:38:22.016874Z","iopub.status.idle":"2024-06-08T02:38:22.177836Z","shell.execute_reply.started":"2024-06-08T02:38:22.016847Z","shell.execute_reply":"2024-06-08T02:38:22.176860Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Translated sentence: A little boy playing with a ball in the snow . <eos>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"You should get a translation similar to the reference after 20 epochs of training.","metadata":{"_uuid":"ce704433-cf9b-4dd0-85ec-8f05d2df792b","_cell_guid":"eef58268-74eb-4401-8ebd-7326faf2f98c","id":"26642c2e","trusted":true}},{"cell_type":"code","source":"","metadata":{"_uuid":"d848b151-106f-497f-8ae9-5c378efc0398","_cell_guid":"3f792288-1188-43f1-81c3-e0fbb72ba423","collapsed":false,"id":"XaB_F-347Vkw","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}